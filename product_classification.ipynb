{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:997)>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:997)>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting es-dep-news-trf==3.4.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/es_dep_news_trf-3.4.0/es_dep_news_trf-3.4.0-py3-none-any.whl (410.2 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 410.2/410.2 MB 3.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy-transformers<1.2.0,>=1.1.2 in ./venv/lib/python3.10/site-packages (from es-dep-news-trf==3.4.0) (1.1.8)\n",
      "Requirement already satisfied: spacy<3.5.0,>=3.4.0 in ./venv/lib/python3.10/site-packages (from es-dep-news-trf==3.4.0) (3.4.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in ./venv/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->es-dep-news-trf==3.4.0) (2.28.1)\n",
      "Requirement already satisfied: pathy>=0.3.5 in ./venv/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->es-dep-news-trf==3.4.0) (0.6.2)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in ./venv/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->es-dep-news-trf==3.4.0) (8.1.5)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in ./venv/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->es-dep-news-trf==3.4.0) (1.0.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./venv/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->es-dep-news-trf==3.4.0) (2.0.7)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in ./venv/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->es-dep-news-trf==3.4.0) (3.0.10)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in ./venv/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->es-dep-news-trf==3.4.0) (4.64.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./venv/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->es-dep-news-trf==3.4.0) (1.0.9)\n",
      "Requirement already satisfied: numpy>=1.15.0 in ./venv/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->es-dep-news-trf==3.4.0) (1.23.4)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in ./venv/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->es-dep-news-trf==3.4.0) (2.0.8)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in ./venv/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->es-dep-news-trf==3.4.0) (2.4.5)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in ./venv/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->es-dep-news-trf==3.4.0) (3.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->es-dep-news-trf==3.4.0) (21.3)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->es-dep-news-trf==3.4.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->es-dep-news-trf==3.4.0) (65.5.0)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in ./venv/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->es-dep-news-trf==3.4.0) (0.4.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./venv/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->es-dep-news-trf==3.4.0) (3.0.8)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in ./venv/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->es-dep-news-trf==3.4.0) (1.9.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in ./venv/lib/python3.10/site-packages (from spacy<3.5.0,>=3.4.0->es-dep-news-trf==3.4.0) (0.10.1)\n",
      "Requirement already satisfied: transformers<4.22.0,>=3.4.0 in ./venv/lib/python3.10/site-packages (from spacy-transformers<1.2.0,>=1.1.2->es-dep-news-trf==3.4.0) (4.21.3)\n",
      "Requirement already satisfied: spacy-alignments<1.0.0,>=0.7.2 in ./venv/lib/python3.10/site-packages (from spacy-transformers<1.2.0,>=1.1.2->es-dep-news-trf==3.4.0) (0.8.6)\n",
      "Requirement already satisfied: torch>=1.6.0 in ./venv/lib/python3.10/site-packages (from spacy-transformers<1.2.0,>=1.1.2->es-dep-news-trf==3.4.0) (1.12.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in ./venv/lib/python3.10/site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->es-dep-news-trf==3.4.0) (3.0.9)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in ./venv/lib/python3.10/site-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->es-dep-news-trf==3.4.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./venv/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4->spacy<3.5.0,>=3.4.0->es-dep-news-trf==3.4.0) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in ./venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->es-dep-news-trf==3.4.0) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->es-dep-news-trf==3.4.0) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->es-dep-news-trf==3.4.0) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->es-dep-news-trf==3.4.0) (2022.9.24)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in ./venv/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->es-dep-news-trf==3.4.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in ./venv/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->es-dep-news-trf==3.4.0) (0.0.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in ./venv/lib/python3.10/site-packages (from transformers<4.22.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->es-dep-news-trf==3.4.0) (0.10.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./venv/lib/python3.10/site-packages (from transformers<4.22.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->es-dep-news-trf==3.4.0) (2022.9.13)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./venv/lib/python3.10/site-packages (from transformers<4.22.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->es-dep-news-trf==3.4.0) (6.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in ./venv/lib/python3.10/site-packages (from transformers<4.22.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->es-dep-news-trf==3.4.0) (0.12.1)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.10/site-packages (from transformers<4.22.0,>=3.4.0->spacy-transformers<1.2.0,>=1.1.2->es-dep-news-trf==3.4.0) (3.8.0)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in ./venv/lib/python3.10/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->es-dep-news-trf==3.4.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.10/site-packages (from jinja2->spacy<3.5.0,>=3.4.0->es-dep-news-trf==3.4.0) (2.1.1)\n",
      "\u001B[38;5;2m✔ Download and installation successful\u001B[0m\n",
      "You can now load the package via spacy.load('es_dep_news_trf')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import plotly.express as px\n",
    "\n",
    "#Libraries for preprocessing\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "import webcolors\n",
    "\n",
    "#Download once if using NLTK for preprocessing\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stops = set(stopwords.words('spanish'))\n",
    "\n",
    "\n",
    "#Libraries for vectorisation\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "#Libraries for clustering\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.cli.download(\"es_dep_news_trf\")\n",
    "nlp = spacy.load(\"es_dep_news_trf\")\n",
    "\n",
    "#Load data set\n",
    "#df = pd.read_csv('Productos con Data Extra.csv', delimiter=';', encoding=\"utf-8\")\n",
    "df = pd.read_excel('Productos con Data Extra.xlsx')\n",
    "df['nombre'] = df['nombre'].astype(str)\n",
    "text1 = df['nombre']\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "outputs": [],
   "source": [
    "from spacy.lang.es.stop_words import STOP_WORDS\n",
    "stop = STOP_WORDS"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "outputs": [],
   "source": [
    "df = pd.read_excel('Productos con Data Extra.xlsx')\n",
    "df['nombre'] = df['nombre'].astype(str)\n",
    "df = df[['nombre']]\n",
    "df['nombre'] = df['nombre'].apply(lambda x: ''.join([i for i in x if not i.isdigit()]))\n",
    "df['nombre'] = df.nombre.str.replace('[#,@,&,°,%,º,/]', '')\n",
    "df['nombre'] = df['nombre'].replace('lí-quido', 'líquido')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "stops = stopwords.words(\"spanish\")\n",
    "\n",
    "def normalize(comment, lowercase, remove_stopwords):\n",
    "    if lowercase:\n",
    "        comment = comment.lower()\n",
    "    comment = nlp(comment)\n",
    "    lemmatized = list()\n",
    "    for word in comment:\n",
    "        if len(word)>3:\n",
    "            if not word.is_punct:\n",
    "                lemma = word.lemma_.strip()\n",
    "                if lemma:\n",
    "                    if not remove_stopwords or (remove_stopwords and lemma not in stops):\n",
    "                        lemmatized.append(lemma)\n",
    "    return \" \".join(lemmatized)\n",
    "\n",
    "\n",
    "df['texto'] = df['nombre'].apply(normalize, lowercase=True, remove_stopwords=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "text1 = df['texto']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Stem and make lower case\n",
    "def stemSentence(sentence):\n",
    "    #stemmer = SnowballStemmer('spanish')\n",
    "    token_words = word_tokenize(sentence)\n",
    "    #stem_sentence = [stemmer.stem(word) for word in token_words]\n",
    "    stem_sentence = [word for word in token_words]\n",
    "    return ' '.join(stem_sentence)\n",
    "text3 = pd.Series([stemSentence(x) for x in text1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Remove colours\n",
    "#colors = list(webcolors.CSS3_NAMES_TO_HEX)\n",
    "#colors = [stemSentence(x) for x in colors if x in ('naranja','frutilla','chocolate','vainilla','limón',               'oliva', 'manzana', 'zero', 'pera', 'color', 'collection', 'collecion','extra', 'pack','oregano')]\n",
    "colors = ['naranja','frutilla','chocolate','vainilla','limón', 'oliva', 'manzana', 'zero', 'pera', 'color', 'collection', 'collecion', 'extra', 'pack','oregano', 'li', 'lí', 'piña', 'fruta', 'frambuesa', 'mango', 'durazno']\n",
    "text4 = [' '.join([x for x in string.split() if x not in colors]) for string in text3]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Bag of words\n",
    "vectorizer_cv = CountVectorizer(analyzer='word')\n",
    "X_cv = vectorizer_cv.fit_transform(text4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#TF-IDF (word level)\n",
    "vectorizer_wtf = TfidfVectorizer(analyzer='word')\n",
    "X_wtf = vectorizer_wtf.fit_transform(text4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "text4"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "matrix = pd.concat([text1, pd.DataFrame(X_cv.toarray(), columns=vectorizer_cv.get_feature_names())],axis=1)\n",
    "matrix[['texto', 'cerveza']]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "matrix"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#TF-IDF (n-gram level)\n",
    "vectorizer_ntf = TfidfVectorizer(analyzer='word',ngram_range=(1,2))\n",
    "X_ntf = vectorizer_ntf.fit_transform(text4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#LDA\n",
    "lda = LatentDirichletAllocation(n_components=30, learning_decay=0.9)\n",
    "X_lda = lda.fit(X_cv)\n",
    "\n",
    "#Plot topics function. Code from: https://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html\n",
    "def plot_top_words(model, feature_names, n_top_words, title):\n",
    "    fig, axes = plt.subplots(6, 5, figsize=(30, 30), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_features_ind = topic.argsort()[:-n_top_words - 1:-1]\n",
    "        top_features = [feature_names[i] for i in top_features_ind]\n",
    "        weights = topic[top_features_ind]\n",
    "\n",
    "        ax = axes[topic_idx]\n",
    "        ax.barh(top_features, weights, height=0.7)\n",
    "        ax.set_title(f'Topic {topic_idx +1}',\n",
    "                     fontdict={'fontsize': 30})\n",
    "        ax.invert_yaxis()\n",
    "        ax.tick_params(axis='both', which='major', labelsize=20)\n",
    "        for i in 'top right left'.split():\n",
    "            ax.spines[i].set_visible(False)\n",
    "        fig.suptitle(title, fontsize=40)\n",
    "    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
    "    plt.show()\n",
    "\n",
    "#Show topics\n",
    "n_top_words = 5\n",
    "feature_names = vectorizer_cv.get_feature_names()\n",
    "plot_top_words(X_lda, feature_names, n_top_words, '')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "     #Fuzzywuzzy\n",
    "X_fuzz = pd.crosstab([text4.index,text4],text4).apply(lambda col: [fuzz.token_sort_ratio(col.name, x)\n",
    "                                                                   for x in col.index.get_level_values(1)])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Test increments of 100 clusters using elbow method\n",
    "sse={}\n",
    "for k in np.arange(100,900,100):\n",
    "    kmeans = KMeans(n_clusters=k, max_iter=1000).fit(X_cv)\n",
    "    sse[k] = kmeans.inertia_\n",
    "plt.plot(list(sse.keys()),list(sse.values()))\n",
    "plt.xlabel('Values for K')\n",
    "plt.ylabel('SSE')\n",
    "plt.show();"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Create 200 clusters\n",
    "kmeans = KMeans(n_clusters=200)\n",
    "kmeans.fit(X_cv)\n",
    "result = pd.concat([text1,pd.DataFrame(X_cv.toarray(),columns=vectorizer_cv.get_feature_names())],axis=1)\n",
    "result['cluster'] = kmeans.predict(X_cv)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Label each cluster with the word(s) that all of its food names have in common\n",
    "clusters = result['cluster'].unique()\n",
    "labels = []\n",
    "for i in range(len(clusters)):\n",
    "    subset = result[result['cluster'] == clusters[i]]\n",
    "    words = ' '.join([x for x in np.where(subset.all()!=0,subset.columns,None) if x and x!='texto' and x!='cluster' and len(x.split()) == 1])\n",
    "    labels.append(words)\n",
    "labels_table = pd.DataFrame(zip(clusters,labels),columns=['cluster','label'])\n",
    "result_labelled = pd.merge(result,labels_table,on='cluster',how='left')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "labels_table"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "result_labelled[['texto', 'label_y']]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Visualise sizes of supermarket categories (manually added to result_labelled) and clean clusters\n",
    "result_summary = pd.pivot_table(result_labelled,index=['label','category'],values=['nombre'],aggfunc='count').reset_index().rename(columns={'Name':'count'})\n",
    "result_treemap = result_summary[(result_summary['label'] != '') & (result_summary['count'] > 1)]\n",
    "fig = px.treemap(result_treemap,path=['category','label'],values='count')\n",
    "fig.show();"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}